{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make all required imports. For this notebook, you need to install the Python package [`efficient-apriori`](https://pypi.org/project/efficient-apriori/). You can add further packages if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "from src.utils import powerset, binary_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Complexity Analysis of Naive Brute Force Approach (6 Points)\n",
    "\n",
    "The most naive approach for mining assocation rules would be generate all possible rules and check if their support and confidence exceed to specified thresholds `minsup` and `minconf`. In the lecture, you have learned that, given $d$ unique items in a dataset of transactions, there are $3^d - 2^{d+1} + 1$ possible rules.\n",
    "\n",
    "**Task 1 (6 points)** Proof that $d$ unique items result in $3^d - 2^{d+1} + 1$ possible rules! (Hint: Write out all possible rules for $d = 2, 3, 4, ...$ items; you should quickly spot the pattern that will allow you to validate the formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "Given d unique items and an itemset L, we need to find all non-empty subsets x that x->L-x, ignoring empty set->L and L->empty set\n",
    "\n",
    "\n",
    "For the left-hand side of rule: consider an itemset has k elements, k: [1, d-1]\n",
    "\n",
    "There are $\\begin{pmatrix} d \\\\ k \\end{pmatrix}$ possible itemsets of size k.\n",
    "\n",
    "For the right-hand side of rule: item of size k can form a rule with the remaining d-k items.\n",
    "\n",
    "An itemset of size k can form a rule with $2^{d-k}-1$ non-empty itemsets.\n",
    "\n",
    "\n",
    "So the total number of possible rules N:\n",
    "\n",
    "$N = \\sum_{k=1}^{d-1}\\begin{pmatrix} d \\\\ k \\end{pmatrix}(2^{d-k} - 1) $\n",
    "$ = \\sum_{k=0}^{d-1}\\begin{pmatrix} d \\\\ k \\end{pmatrix}(2^{d-k} - 1) - (2^d - 1) $\n",
    "$ = (\\sum_{k=0}^{d}\\begin{pmatrix} d \\\\ k \\end{pmatrix}2^{d-k}) - (\\sum_{k=0}^{d}\\begin{pmatrix} d \\\\ k \\end{pmatrix}) - (2^d - 1)$\n",
    "$ = 3^d - 2^d - 2^d + 1$\n",
    "$ = 3^d - 2^{d+1} + 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Finding Association Rules - Implementation (14 Points)\n",
    "\n",
    "Your task is to implement association rule mining. Of course, we do not use the naive approach discussed above where we would generate and check $3^d - 2^{d+1} + 1$, with $d$ being the number of unique items in a dataset.\n",
    "\n",
    "In the lecture, we saw that an association rule $X\\Rightarrow Y$ has only sufficient support if the itemset $X\\cup Y$ has sufficient support -- that is, $X\\cup Y$ is a frequent itemset. Although there are still $2^d-1$ possible itemset we need to check, this brute-force approach for Frequent Itemset Generation is easy to implement and gives a better understanding of the complexity of the task of association rule mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxillary Methods\n",
    "\n",
    "We provide you with two methods to make the implementation tasks easier for you\n",
    "\n",
    "Given a set of items, `powerset()` returns all possible subset of items with a specified minimum and maximum length. For example, you can use this method to generate all itemsets for a transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a',)\n",
      "('b',)\n",
      "('c',)\n",
      "('a', 'b')\n",
      "('a', 'c')\n",
      "('b', 'c')\n",
      "('a', 'b', 'c')\n"
     ]
    }
   ],
   "source": [
    "for subset in powerset(('c', 'b', 'a'), min_len=1, max_len=3):\n",
    "    print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of items, `binary_split()` return all combinations of how the input set can be split into 2 non-empty subsets (where the union of both subsets for the input set). It's easy to see that you can use this to get from an itemset to all possible association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a',) => ('b', 'c')\n",
      "('b',) => ('a', 'c')\n",
      "('c',) => ('a', 'b')\n",
      "('a', 'b') => ('c',)\n",
      "('a', 'c') => ('b',)\n",
      "('b', 'c') => ('a',)\n"
     ]
    }
   ],
   "source": [
    "for X, Y in binary_split(('b','c','a')):\n",
    "    print('{} => {}'.format(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Note that we implement each transaction and each itemset as Python `tuple` and not as a Python `set`. This makes the implementation of the algorithm much easier as tuples can be used as keys in dictionaries (sets cannot). To ensure that you don't have to worry about the order of items in a tuple, both `powerset` and `binary_split` return subsets or pairs of subsets where the items are sorted -- check the example outputs above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Dataset\n",
    "\n",
    "The following dataset with 5 transaction and 6 different items is directly taken from the lecture slides. This should make it easier to test your implementation. You also only implement the brute-force approach which would not perform over the real-world dataset we later look at later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_demo = [\n",
    "    ('bread', 'yogurt'),\n",
    "    ('bread', 'milk', 'cereal', 'eggs'),\n",
    "    ('yogurt', 'milk', 'cereal', 'cheese'),\n",
    "    ('bread', 'yogurt', 'milk', 'cereal'),\n",
    "    ('bread', 'yogurt', 'milk', 'cheese')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, we talked about the completely naive approach to generate and check all possible association rules given a set of $d$ unique items. This would result in $3^d - 2^{d+1} + 1$ rules to generate and check (you just proofed this). As a first ways to address the $O(3^d)$ complexity was to decouple the calculation of support and confidence which allowed us to split the task of finding association rules into two parts:\n",
    "\n",
    "* **(1) Frequent Itemset Generation:** Identify all frequent itemsets -- that is, itemsets with a support greater or equal to `minsup`. The observation was to only frequent itemset allow to derive rules with sufficient support\n",
    "    \n",
    "* **(2) Association Rules Generation:** Given all frequent itemsets, generate all possible rules and check if their confidence is above `minconf`.\n",
    "\n",
    "We have seen in the lecture that Step (2) is arguably less problematic as the information required to calculate the confidence if a rule has already been generated during (1). We therefore focused on the complexity of Frequent Itemset Generation. Again, we first looked at the naive approach to check all possible itemsets if their support is above minsup. Here, give $d$ unique items, that the number of possible itemsets is $2^d-1$.\n",
    "\n",
    "In the following, you will implement Assocition Rule Ming using this naive approach. Although this approach is with $O(2^d) still exponential, it's implementation is straightforward, provides a better understanding of support and confidence, as well as helps to appreciate the complexity that call for more advanced methods such as the Apriori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Step 1: Frequent Itemset Generation (Brute Force) (7 Points)\n",
    "\n",
    "Implement the brute force approach for Frequent Itemset Generation -- i.e., generate all possible itemsets for each transaction and calculate the overall support -- using the template for method `find_frequent_itemsets()` below. The expected format of the output is given in the comments below. The auxiliary method `powerset()` should help for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frequent_itemsets(transactions, minsup):\n",
    "    \n",
    "    num_transactions = len(transactions)\n",
    "    \n",
    "    #########################################################################################\n",
    "    # Step 1: Count the number of occurences of all possible itemset\n",
    "    #########################################################################################\n",
    "\n",
    "    # Create a dictionary to keep track of the support counts for each itemset\n",
    "    # e.g., support_counts = {(a,): 4, (b,): 20, (c,): 5, (a, c): 2, ...}\n",
    "    support_counts = {}\n",
    "    \n",
    "    ### Your code starts here ###############################################################\n",
    "\n",
    "    for transaction in transactions:\n",
    "        for itemset in powerset(transaction, min_len=1, max_len=len(transaction)):\n",
    "          if itemset not in support_counts:\n",
    "            support_counts[itemset] = 0\n",
    "          support_counts[itemset] += 1\n",
    "    #print(support_counts)\n",
    "        \n",
    "        \n",
    "    ### Your code ends here #################################################################\n",
    "                \n",
    "                \n",
    "    #########################################################################################\n",
    "    # Step 2: Filter all itemset with a support >= minsup ==> frequent item sets\n",
    "    #########################################################################################\n",
    "    \n",
    "    # In the end, frequent_itemsets as dictionary (key = itemset, value = support)\n",
    "    # e.g., frequent_itemsets = {(a,): 0.8, (b,): 0.6, (c,): 0.8, (a, c): 0.4, ...}\n",
    "    frequent_itemsets = {}\n",
    "\n",
    "    ### Your code starts here ###############################################################\n",
    "\n",
    "    for key, value in support_counts.items():\n",
    "      sup = value/num_transactions\n",
    "      if sup >= minsup:\n",
    "        frequent_itemsets[key] = sup\n",
    "    \n",
    "    ### Your code ends here #################################################################\n",
    "\n",
    "    # Return frequent itemsets (incl. their support)\n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation of `find_frequent_itemsets()`. The results should match with the examples on the lecture slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bread',) 0.8\n",
      "('yogurt',) 0.8\n",
      "('bread', 'yogurt') 0.6\n",
      "('cereal',) 0.6\n",
      "('milk',) 0.8\n",
      "('bread', 'milk') 0.6\n",
      "('cereal', 'milk') 0.6\n",
      "('milk', 'yogurt') 0.6\n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets = find_frequent_itemsets(transactions_demo, 0.6)\n",
    "\n",
    "for itemset, support_count in frequent_itemsets.items():\n",
    "    print(itemset, support_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Step 2: Find Association Rules (7 Points)\n",
    "\n",
    "Complete method `find_association_rules()` to find all association rules with sufficient support and confidence for a given set of transactions. This method uses `find_frequent_itemsets` to first compute all frequent itemsets. Again, the expected format of the output is given in the comments below. The auxiliary method `binary_split()` should help for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_association_rules(transactions, minsup=0.0, minconf=0.0):\n",
    "\n",
    "    # Perform Step 1: Frequent Itemset Generation\n",
    "    frequent_itemsets = find_frequent_itemsets(transactions, minsup)\n",
    "    \n",
    "    # In the end, rules is a dictionary (key = (X, Y), value = (support, confidence, lift))\n",
    "    # e.g., {(('cereal',), ('milk,')): (0.6, 1.0, 1.25), ...}\n",
    "    rules = {}\n",
    "    \n",
    "    ### Your code starts here ###############################################################\n",
    "    for itemset,sup in frequent_itemsets.items():\n",
    "      for X, Y in binary_split(itemset):\n",
    "        #print('{} => {}'.format(X, Y))\n",
    "\n",
    "        sup_X = frequent_itemsets[X]\n",
    "        sup_Y = frequent_itemsets[Y]\n",
    "        conf = sup / sup_X\n",
    "        lift = sup / (sup_X * sup_Y)\n",
    "        if conf >= minconf:\n",
    "          rules[X,Y] = tuple((sup, conf, lift))\n",
    "        #print(conf)\n",
    "        \n",
    "        \n",
    "    ### Your code ends here #################################################################\n",
    "                \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation of `find_association_rules()`. The results should match with the examples on the lecture slides. You can also use the `efficient-apriori` package on the toy data to see if your implementation returns the same results; see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule [('cereal',) => ('milk',)] (support: 0.6, confidence: 1.0, lift: 1.25)\n"
     ]
    }
   ],
   "source": [
    "rules = find_association_rules(transactions_demo, minsup=0.6, minconf=1.0)\n",
    "    \n",
    "for (X, Y), (sup, conf, lift) in rules.items():\n",
    "    print('Rule [{} => {}] (support: {}, confidence: {}, lift: {})'.format(X, Y, sup, conf, lift))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with `efficient-apriori` package\n",
    "\n",
    "You can run the apriori algorithm over the demo data to check if your implementation is correct. Try different values for the parameters `minsup` and `minconf` and compare the results. Note that the order of the returned association rules might differ between your implementation and the apriori one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule [('cereal',) => ('milk',)] (support: 0.6, confidence: 1.0, lift: 1.25)\n"
     ]
    }
   ],
   "source": [
    "_, rules = apriori(transactions_demo, min_support=0.6, min_confidence=1.0, max_length=4)\n",
    "\n",
    "for r in rules:\n",
    "    print('Rule [{} => {}] (support: {}, confidence: {}, lift: {})'.format(r.lhs, r.rhs, r.support, r.confidence, r.lift))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Association Rule Mining over Real-World Data (30 Points)\n",
    "\n",
    "Although you now implemented association rule mining yourself, the brute-force approach for Frequent Rule Mining does not scale for real-world dataset. With even small real-world datasets containing several thousands of unique items, number of $2^d-1$ possible itemsets quickly explodes. You can try running your implementation on the retail dataset below, but chances are very high that the notebook will crash :).\n",
    "\n",
    "In the following, we therefore use the `efficient-apriori` package that implements the Apriori algorihtm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "This Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail. The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers (Source: https://archive.ics.uci.edu/ml/machine-learning-databases/00502/)\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "* **Invoice**: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'C', it indicates a cancellation.\n",
    "* **StockCode**: Product (item) code. Nominal. A 5-digit integral number and an optional letter, uniquely assigned to each distinct product. The optional letter is used to further distinguish products that only differn w.r.t., for example, their color; see Row 2 and 3 below. \n",
    "* **Description**: Product (item) name. Nominal.\n",
    "* **Quantity**: The quantities of each product (item) per transaction. Numeric.\n",
    "* **InvoiceDate**: Invice date and time. Numeric. The day and time when a transaction was generated.\n",
    "* **Price**: Unit price. Numeric. Product price per unit in sterling.\n",
    "* **CustomerID**: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n",
    "\n",
    "**Important:** \n",
    "* We treat products that only differ in the optional letter as different products. For example, 79323P (PINK CHERRY LIGHTS) and 79323W (WHITE CHERRY LIGHTS) are different products.\n",
    "* Although the basic algorithm for Association Rule Mining we have seen in the lecture does not consider further information such as the quantity, price of items, the time of transaction, or customer information, these attributes might still affect the data cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Customer ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489434</td>\n",
       "      <td>85048</td>\n",
       "      <td>15CM CHRISTMAS GLASS BALL 20 LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>12/01/09 07:45 AM</td>\n",
       "      <td>6.95</td>\n",
       "      <td>13085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323P</td>\n",
       "      <td>PINK CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>12/01/09 07:45 AM</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323W</td>\n",
       "      <td>WHITE CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>12/01/09 07:45 AM</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489434</td>\n",
       "      <td>22041</td>\n",
       "      <td>RECORD FRAME 7\" SINGLE SIZE</td>\n",
       "      <td>48</td>\n",
       "      <td>12/01/09 07:45 AM</td>\n",
       "      <td>2.10</td>\n",
       "      <td>13085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489434</td>\n",
       "      <td>21232</td>\n",
       "      <td>STRAWBERRY CERAMIC TRINKET BOX</td>\n",
       "      <td>24</td>\n",
       "      <td>12/01/09 07:45 AM</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13085.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Invoice StockCode                          Description  Quantity  \\\n",
       "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
       "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
       "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
       "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
       "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
       "\n",
       "         InvoiceDate  Price  Customer ID  \n",
       "0  12/01/09 07:45 AM   6.95      13085.0  \n",
       "1  12/01/09 07:45 AM   6.75      13085.0  \n",
       "2  12/01/09 07:45 AM   6.75      13085.0  \n",
       "3  12/01/09 07:45 AM   2.10      13085.0  \n",
       "4  12/01/09 07:45 AM   1.25      13085.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_retail = pd.read_csv('data/online-retail.csv')\n",
    "\n",
    "df_retail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 525461 entries, each with 7 attributes.\n"
     ]
    }
   ],
   "source": [
    "num_entries, num_attributes = df_retail.shape\n",
    "\n",
    "print('There are {} entries, each with {} attributes.'.format(num_entries, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since association rules will only have stock codes as antecedents and consequents, they are not easy to interpret. To quickly map stock codes to descriptions, we can generate a dictionary for later use. This might take a couple of seconds. \n",
    "\n",
    "**Note:** This is important for completing the notebook, but help to better interpret the rules. You can also check out the lecture notebook about Association Rule Mining to see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 s, sys: 174 ms, total: 23 s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "code2desc = { row['StockCode']:row['Description'] for  idx, row in df_retail.iterrows() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example for using `code2desc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The item (description) for 85048 is: 15CM CHRISTMAS GLASS BALL 20 LIGHTS\n"
     ]
    }
   ],
   "source": [
    "stock_code = '85048'\n",
    "\n",
    "print('The item (description) for {} is: {}'.format(stock_code, code2desc[stock_code]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Exploratory Data Analysis & Data Cleaning (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 a) Find and Remove \"Dirty\" Records (10)\n",
    "\n",
    "If you check the dataset against its description -- see the attribute information above -- you will notice that many records are \"dirty\", meaning they are not in th expected format. In the following, **identify at least 3 cases** of dirty records, and clean your data accordingly.\n",
    "\n",
    "\n",
    "**Important:** \n",
    "* Recall from the lecture that data cleaning often involves to make certain decisions. As such, you might come up with different steps than other students. This is OK as long as you can reasonably justify your steps.\n",
    "* A transaction generally contains multiple rows/records. If at least one row/record of a transaction needs to be removed because it's dirty, you should remove all records of the same transaction to avoid inconsistencies.\n",
    "* Perform the data cleaning on a copy of the original dataset `df_retail_cleaned`; see code cell below. Later tasks will work on the original dataset `df_retail` to ensure that the result are consistent and do not depend on your choice of data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please provide your answer below. It should list the different cases of \"dirty\" records you have identified and briefly discuss which data cleaning steps you can and/or need to perform to address those records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "\n",
    "\n",
    "(1) Incomplete Data: missing values\n",
    "\n",
    "eg. Empty data in \"Customer ID\".\n",
    "\n",
    "data cleaning step:\n",
    "- check columns that contains empty data using is.na() method\n",
    "- remove records that contain missing values in column like 'Customer ID'\n",
    "\n",
    "(2)check duplicate values\n",
    "\n",
    "Some records might be duplicated. If so, keep only the first record using python drop_duplicates() function.\n",
    "\n",
    "(3) Incorrect data 'Stockcode'\n",
    "\n",
    "eg. 'StockCode' = 'M' while 'StockCode' should be a 5-digit integral number and an optional letter\n",
    "\n",
    "data cleaning step: remove such records which are not '5-digit integral number and an optional letter'\n",
    "\n",
    "(4) Incorrect data 'Quantity'\n",
    "\n",
    "eg. 'Quantity' = -770 while 'Quantity' shoule be Integers greater than 0\n",
    "\n",
    "data cleaning step:\n",
    "\n",
    "remove such records where their 'Quantity' < 0\n",
    "\n",
    "OR the negative values are probably mistakes, we can turn it into possitive value using abs() method (getting absolute value)\n",
    "\n",
    "(5) Incorrect data 'Price'\n",
    "\n",
    "eg. 'Price' = 0 while the product prive per unit should not be 0 in general\n",
    "\n",
    "data cleaning step: we can remove record where their 'Price' <= 0; and since such transaction contains dirty data, we can also remove all same transaction.\n",
    "\n",
    "(6)(optional) 'Invoice' that starts with 'C' actually indicates cancellations, so we might want to get rid of it.\n",
    "\n",
    "Data cleaning step: remove record with 'Invoice' starting with C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code cell below to actually implement your steps to identify \"dirty\" records as well as your steps to clean the data. The results should back up your answer above. Feel free to split the cell into multiple code cells to improve organization (not a must, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check empty data:\n",
      "Invoice             0\n",
      "StockCode           0\n",
      "Description      2928\n",
      "Quantity            0\n",
      "InvoiceDate         0\n",
      "Price               0\n",
      "Customer ID    107927\n",
      "dtype: int64\n",
      "After removing nan data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(417534, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first create a copy of the dataset and use this one to clean the data.\n",
    "df_retail_cleaned = df_retail.copy()\n",
    "\n",
    "\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "# 1. check and remove missing values\n",
    "print('Check empty data:')\n",
    "print(df_retail_cleaned.isna().sum()) #check empty data\n",
    "# 'Customer Id' contains ‘nan’ values which should drop\n",
    "df_retail_cleaned = df_retail_cleaned[df_retail_cleaned['Customer ID'] != 'nan']\n",
    "df_retail_cleaned.dropna(inplace= True)\n",
    "print('After removing nan data:')\n",
    "df_retail_cleaned.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417534, 7)\n",
      "(410763, 7)\n"
     ]
    }
   ],
   "source": [
    "# 2. detect duplicate values\n",
    "print(df_retail_cleaned.shape)\n",
    "print(df_retail_cleaned.drop_duplicates().shape) #smaller than df_retail_cleaned, which means there are duplicate values\n",
    "# drop duplicate entries\n",
    "df_retail_cleaned.drop_duplicates(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408090, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. check incorrect data for 'Stockcode'\n",
    "\n",
    "#'StockCode' should only contain a 5-digit integral number and an optional letter\n",
    "correct_stockcode = ((df_retail_cleaned['StockCode'].str.isdigit()) & (df_retail_cleaned['StockCode'].str.len() == 5)) | ((df_retail_cleaned['StockCode'].str.len() == 6) & (df_retail_cleaned['StockCode'].str[:5].str.isdigit()) & (df_retail_cleaned['StockCode'].str[-1].str.isalpha()))\n",
    "df_retail_cleaned = df_retail_cleaned[correct_stockcode]\n",
    "df_retail_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check incorrect for column Quantity:\n",
      "(9268, 7)\n"
     ]
    }
   ],
   "source": [
    "# 4. check incorrect data for 'Quantity'\n",
    "print('Check incorrect for column Quantity:')\n",
    "print(df_retail_cleaned[df_retail_cleaned['Quantity'] < 0].shape)\n",
    "# assuming negative quantity is a mistake, turn them into positive integers (absolute value)\n",
    "df_retail_cleaned['Quantity'] = df_retail_cleaned['Quantity'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check incorrect column price:\n",
      "(28, 7)\n",
      "After this step:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(408062, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Incorrect data 'Price'\n",
    "print('Check incorrect column price:')\n",
    "print(df_retail_cleaned[df_retail_cleaned['Price'] <= 0].shape)\n",
    "df_retail_cleaned = df_retail_cleaned[df_retail_cleaned['Price'] > 0] #drop all records where their price <= 0\n",
    "\n",
    "# because 'price' is dirty, drop all records of the same transaction to avoid inconsistencies\n",
    "dirty_price_invoice = list(df_retail_cleaned[df_retail_cleaned['Price'] <= 0]['Invoice'])\n",
    "#print(dirty_price_invoice)\n",
    "df_retail_cleaned = df_retail_cleaned[~df_retail_cleaned.Invoice.isin(dirty_price_invoice)] #drop all corresponding transactions\n",
    "print('After this step:')\n",
    "df_retail_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Drop cancellations transactions (optional)\n",
    "\n",
    "df_retail_cleaned = df_retail_cleaned[~df_retail_cleaned['Invoice'].str.contains('C')]\n",
    "# 'Invoice' that starts with 'C' actually indicates cancellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing, There are now 398794 entries.\n"
     ]
    }
   ],
   "source": [
    "### Your code ends here #################################################################\n",
    "\n",
    "\n",
    "print('After preprocessing, There are now {} entries.'.format(df_retail_cleaned.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1b) Basic Facts about the Dataset (8 Points)\n",
    "\n",
    "The following tasks are about getting basic insights into the dataset. As the data preprocessing steps you choose to perform might effect the results of these tasks, please use the original data stored in `df_retail`. \n",
    "\n",
    "Use the code cell below to actually implement your steps that enabled you to answer the questions. This allows for a fairer grading even if your answers are not (exactly) correct. Again, please use the uncleaned dataset `df_retail` to ensure consistent results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answer for (1)~(8).\n",
    "\n",
    "| No. | Question                                                                                                   | Answer       |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------|\n",
    "| 1)  | Starting date of the dataset?                                                                              |2009-12-01|\n",
    "| 2)  | Ending date of the dataset?                                                                                |2010-12-09|\n",
    "| 3)  | Number of customers?                                                                                       |4383|\n",
    "| 5)  | Number of unique items?                                                                         |4632|\n",
    "| 4)  | Number of transactions (incl. canceled transactions)?                                                                                    |28816|\n",
    "| 6)  | Number of transactions Customer ID 17850 has made (incl. canceled transactions)?                                                         |158|\n",
    "| 7)  | Which customer (ID) has made the most transactions (incl. canceled transactions) and how many?                                                        |14911, 270|\n",
    "| 8)  | What is the item ID of the best-seller (best-seller = item with the highest sales volume)? |21212|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-12-01 07:45:00\n",
      "2010-12-09 20:01:00\n",
      "4383\n",
      "4632\n",
      "28816\n",
      "158\n",
      "which customer(ID) has made the most transactions: \n",
      "14911.0\n",
      "how many:\n",
      "270\n",
      "The best-seller is (Item ID): \n",
      "21212\n",
      "The highest sales volume is: \n",
      "59411\n"
     ]
    }
   ],
   "source": [
    "### Your code starts here ###############################################################\n",
    "\n",
    "#1)\n",
    "print(pd.to_datetime(df_retail['InvoiceDate']).min()) #starting date\n",
    "#2)\n",
    "print(pd.to_datetime(df_retail['InvoiceDate']).max()) #ending date\n",
    "#3)\n",
    "print(df_retail['Customer ID'].nunique())\n",
    "#5)\n",
    "print(df_retail['StockCode'].nunique())\n",
    "#4)\n",
    "print(df_retail['Invoice'].nunique())\n",
    "#6)\n",
    "print(df_retail[df_retail['Customer ID'] == 17850]['Invoice'].nunique())\n",
    "#7)\n",
    "#print(df_retail.groupby('Customer ID')['Invoice'].nunique().sort_values(ascending=False))\n",
    "print('which customer(ID) has made the most transactions: ')\n",
    "print(df_retail.groupby('Customer ID')['Invoice'].nunique().idxmax())\n",
    "print('how many:')\n",
    "print(df_retail.groupby('Customer ID')['Invoice'].nunique().max())\n",
    "#8)\n",
    "print('The best-seller is (Item ID): ')\n",
    "#total_amount = df_retail['Quantity'] * df_retail['Price']\n",
    "print(df_retail.groupby(['StockCode']).sum()['Quantity'].idxmax())\n",
    "print('The highest sales volume is: ')\n",
    "print(df_retail.groupby(['StockCode']).sum()['Quantity'].max())\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 c) Complexity Analysis fo Brute-Force Approach (2 Points)\n",
    "\n",
    "We know that our brute-force implementation for Frequent Itemset Generation has to check $2^d-1$ itemsets, and we know the number $d$ of unique items from our EDA. Suppose we can count $2^{36}$ itemsets per second.  Will we complete the counting before the sun burns out (the sun has another $5\\cdot 10^9$ years to burn)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "No, we will not complete the counting before the sun burns out.\n",
    "\n",
    "Reason:\n",
    "\n",
    "how many seconds before the sun burns out:\n",
    "secs = 365 * 24 * 60 *60 = 31,536,000 = 3.1536 * 10^7\n",
    "\n",
    "So how many counts we could complete by then:\n",
    "counts = $2^{36} * 3.1536 * 10^7 * 5 * 10^9 = 2^{36} * 1.5768 * 10^{17}$\n",
    "\n",
    "how many unique items (known from 3.1 EDA): 4632\n",
    "\n",
    "Then we need to check $2^{4632} -1$ itemsets. Exponentially, it is far larger than the counts we can calculate before the sun burns out.\n",
    "\n",
    "So we will not complete the counting before sun burns out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Running the Apriori Algorithm (10 Points)\n",
    "\n",
    "The `efficient-apriori` package assume as input a list of transactions; see `transactions_demo`. However, right now we have all transactions in table-like format. We therefore need to transform the data. Note that using the `StockCode` to represent an item will suffice; we don't need to whole description here. It also saves memory as stock codes are typically much shorter then descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_retail = df_retail.groupby(['Invoice']).agg({'StockCode': tuple})['StockCode'].to_list()\n",
    "\n",
    "#\n",
    "# Output format\n",
    "#\n",
    "# transactions_retail = [\n",
    "#     (22554, 82494L, 21975),    \n",
    "#     (21175, 84991, 85099F, 85099B),\n",
    "#     (85099B, 21930),\n",
    "#     ...\n",
    "#]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** You can check the lecture notebook for Association Rules to see examples using the `efficient-apriori` package, and how to evaluate results. This should help with the following tasks and questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 a) (1 Point)** Run efficient-apriori in python with min_support=0.5%, min_confidence=20%, max_length=4. Write down the rule with the highest lift (denoted as `r1`, e.g., `r1 = ( (A, B), (C,) )` to represent the rule $\\{A,B\\}\\Rightarrow \\{C\\}$, where A, B and C are stock codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{22521, 22522} -> {22520, 22523} (conf: 0.924, supp: 0.005, lift: 155.628, conv: 12.999)\n",
      "['CHILDS GARDEN TROWEL PINK', 'CHILDS GARDEN FORK BLUE '] => ['CHILDS GARDEN TROWEL BLUE ', 'CHILDS GARDEN FORK PINK'] -- lift: 155.62820777433782\n",
      "CPU times: user 16min, sys: 2.23 s, total: 16min 2s\n",
      "Wall time: 16min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "itemsets, rules = apriori(transactions_retail, min_support=0.005, min_confidence=0.2, max_length=4)\n",
    "rules_filtered = rules\n",
    "top_rule = sorted(rules_filtered, key=lambda rule: rule.lift, reverse=True)[0]\n",
    "print(top_rule)\n",
    "\n",
    "antecedent = [ code2desc[c] for c in top_rule.lhs]\n",
    "consequent = [ code2desc[c] for c in top_rule.rhs]\n",
    "print('{} => {} -- lift: {}'.format(antecedent, consequent, top_rule.lift))\n",
    "\n",
    "r1 = tuple((top_rule.lhs, top_rule.rhs))\n",
    "\n",
    "r1_lift = top_rule.lift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 b) (1 Point)** Run efficient-apriori in python with min_support=1%, min_confidence=20%, max_length=4. Write down the rule with the highest lift (denoted as `r2`, e.g., `r2 = ( (A, B), (C,) )` to represent the rule $\\{A,B\\}\\Rightarrow \\{C\\}$, where A, B and C are stock codes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{22699} -> {22697} (conf: 0.741, supp: 0.010, lift: 53.631, conv: 3.804)\n",
      "['ROSES REGENCY TEACUP AND SAUCER '] => ['GREEN REGENCY TEACUP AND SAUCER'] -- lift: 53.63111855574167\n",
      "CPU times: user 4min 17s, sys: 623 ms, total: 4min 18s\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "itemsets, rules = apriori(transactions_retail, min_support=0.01, min_confidence=0.2, max_length=4)\n",
    "rules_filtered = rules\n",
    "top_rule = sorted(rules_filtered, key=lambda rule: rule.lift, reverse=True)[0]\n",
    "print(top_rule)\n",
    "\n",
    "antecedent = [ code2desc[c] for c in top_rule.lhs]\n",
    "consequent = [ code2desc[c] for c in top_rule.rhs]\n",
    "print('{} => {} -- lift: {}'.format(antecedent, consequent, top_rule.lift))\n",
    "\n",
    "\n",
    "\n",
    "r2 = tuple((top_rule.lhs, top_rule.rhs))\n",
    "\n",
    "r2_lift = top_rule.lift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 c) (1 Point)** Run efficient-apriori in python with min_support=0.5%, min_confidence=40%, max_length=4. Write down the rule with the highest lift (denoted as `r3`, e.g., `r3 = ( (A, B), (C,) )` to represent the rule $\\{A,B\\}\\Rightarrow \\{C\\}$, where A, B and C are stock codes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{22521, 22522} -> {22520, 22523} (conf: 0.924, supp: 0.005, lift: 155.628, conv: 12.999)\n",
      "['CHILDS GARDEN TROWEL PINK', 'CHILDS GARDEN FORK BLUE '] => ['CHILDS GARDEN TROWEL BLUE ', 'CHILDS GARDEN FORK PINK'] -- lift: 155.62820777433782\n",
      "CPU times: user 16min 21s, sys: 2.74 s, total: 16min 24s\n",
      "Wall time: 16min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "itemsets, rules = apriori(transactions_retail, min_support=0.005, min_confidence=0.4, max_length=4)\n",
    "rules_filtered = rules\n",
    "top_rule = sorted(rules_filtered, key=lambda rule: rule.lift, reverse=True)[0]\n",
    "print(top_rule)\n",
    "\n",
    "antecedent = [ code2desc[c] for c in top_rule.lhs]\n",
    "consequent = [ code2desc[c] for c in top_rule.rhs]\n",
    "print('{} => {} -- lift: {}'.format(antecedent, consequent, top_rule.lift))\n",
    "\n",
    "r3 = tuple((top_rule.lhs, top_rule.rhs))\n",
    "\n",
    "r3_lift = top_rule.lift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 d) (1 Point)** Run efficient-apriori in python with min_support=1%, min_confidence=40%, max_length=4. Write down the rule with the highest lift (denoted as `r4`, e.g., `r4 = ( (A, B), (C,) )` to represent the rule $\\{A,B\\}\\Rightarrow \\{C\\}$, where A, B and C are stock codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{22699} -> {22697} (conf: 0.741, supp: 0.010, lift: 53.631, conv: 3.804)\n",
      "['ROSES REGENCY TEACUP AND SAUCER '] => ['GREEN REGENCY TEACUP AND SAUCER'] -- lift: 53.63111855574167\n",
      "CPU times: user 4min 26s, sys: 598 ms, total: 4min 26s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "itemsets, rules = apriori(transactions_retail, min_support=0.01, min_confidence=0.4, max_length=4)\n",
    "rules_filtered = rules\n",
    "top_rule = sorted(rules_filtered, key=lambda rule: rule.lift, reverse=True)[0]\n",
    "print(top_rule)\n",
    "\n",
    "antecedent = [ code2desc[c] for c in top_rule.lhs]\n",
    "consequent = [ code2desc[c] for c in top_rule.rhs]\n",
    "print('{} => {} -- lift: {}'.format(antecedent, consequent, top_rule.lift))\n",
    "\n",
    "r4 = tuple((top_rule.lhs, top_rule.rhs))\n",
    "\n",
    "r4_lift = top_rule.lift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 e) (3 Points)** You must have noticed numerous differences between the 4 runs in a)-d). List at least 3 differences you have found. You may want to consider the elapsed time and the quality of the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "(1) different parameters: different min confidence and min support setting\n",
    "(2) different running time\n",
    "(3) result in different set of rules & different best(highest lift) rule\n",
    "(4) different quality of top rules (different confidence, support, lift of rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 f) (3 Points)** From your observation, what are the effects of increasing/reducing `min_support` and `min_confidence`? Support your answer with evidence. You can perform more runs of efficient-apriori with different paramter settings, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Your Answer:**\n",
    "\n",
    "(1)increasing/reducing `min_support` :\n",
    "\n",
    "- increase 'min_support', the elapsed time will decrease.\n",
    "\n",
    "Evidence: the running time of r3 and r4, with the same min_confidence of 0.4, when min_support = 0.005, the Wall times is about 16 mins; when min_support = 0.01, the Wall times is about 4 mins.\n",
    "\n",
    "We can found the similar pattern when finding r1 and r2. With the same min_confidence of 0.2, when min_support = 0.005, the Wall times is about 16 mins; when min_support = 0.01, the Wall times is about 4 mins.\n",
    "\n",
    "- increase 'min_support', the best rule found will change: the lift of best rule is decreased.\n",
    "\n",
    "Evidence: the result of r3,r3_lift and r4,r4_lift. With the same min_confidence of 0.4, when min_support = 0.005, the found rule is {22521, 22522} -> {22520, 22523}, with lift = 155.628; when min_support = 0.01, the fould rule is {22699} -> {22697}, with lift = 53.631. r3_lift > r4_lift\n",
    "\n",
    "We can found the similar pattern in the result of r1, r1_lift and r1, r2_lift. With the same min_confidence of 0.2, when min_support = 0.005, the found rule is {22521, 22522} -> {22520, 22523}, with lift = 155.628; when min_support = 0.01, the fould rule is {22699} -> {22697}, with lift = 53.631. r1_lift > r2_lift\n",
    "\n",
    "(2)increasing/reducing `min_confidence` :\n",
    "\n",
    "- increase 'min_confidence', the elapsed time slightly increase.\n",
    "\n",
    "Evidence: the running time of r2 and r4, with the same min_support of 0.01, when min_confidence = 0.2, the Wall times is 4min 19s; when min_confidence = 0.4, the Wall times is about 4min 28s (slightly increase but no big difference).\n",
    "\n",
    "We can found the similar pattern when finding r1 and r3. With the same min_support of 0.005, when min_confidence = 0.2, the Wall times is 16min 3s; when min_confidence = 0.4, the Wall times is about 16min 25s (slightly increase but no big difference).\n",
    "\n",
    "The change of running time brought by increasing/reducing 'min_confidence' is not as significant as increasing/reducing 'min_support'.\n",
    "\n",
    "(The stage 1 of apriori algorithm using 'min_confidence' will affect running time much more significantly comparing with stage 2 using min_confidence.)\n",
    "\n",
    "- increase 'min_confidence', the same best rule was found.\n",
    "\n",
    "With the same min_support and different min_confidence, the found r1 and r3 are the same rule; So as r2 and r4.\n",
    "\n",
    "(Tried using min_support = 0.01, min_confidence = 0.6, still finding the same best rule as r2 and r4.)\n",
    "\n",
    "- increase 'min_confidence', the size of resulting rule set will decrease.\n",
    "\n",
    "Evidence: consider the finding of r2 and r4, with the same min_support of 0.01, when min_confidence = 0.2, 405 rules were found as the result using len(rules); when min_confidence = 0.4, only 200 rules were found."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
